{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading file and convert to NUMPY array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"*All_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"start_date\", \"end_date\"]] = df[[\"start_date\", \"end_date\"]].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df['duration'].apply(lambda i: dt.timedelta(days=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a) # each column is a \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how to process matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating the data by looping nested, we can use the transpose (convert rows to columns and vice-versa) to calculate the matrix.\n",
    "\n",
    "BUT... we have to adjust our data:\n",
    "\n",
    "- We have to split our data for each column (start_date, end_date, duration and id)\n",
    "- After splintting the data, we have to make sure the shape is (n x 1), in order to calculate the TRANSPOSE\n",
    "- Then we can apply our process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,0].shape # wrong shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,1][np.newaxis].shape # right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,1][np.newaxis].T.shape # applying the transpose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "# a0 = a[:150,0][np.newaxis] #start_date\n",
    "# a1 = a[:150,1][np.newaxis] #end_date\n",
    "# a2 = a[:150,2][np.newaxis] #duration\n",
    "# a3 = a[:150,3][np.newaxis] #id\n",
    "\n",
    "##### UNCOMMENT BELOW TO RUN ALL DATA !!!#####\n",
    "a0 = a[:,0][np.newaxis] #start_date\n",
    "a1 = a[:,1][np.newaxis] #end_date\n",
    "a2 = a[:,2][np.newaxis] #duration\n",
    "a3 = a[:,3][np.newaxis] #id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the conditions\n",
    "> Connectivity between catchments is defined by drought events. \n",
    "If a drought event in a given catchment has a temporal overlap of 50% or more with an event in another catchment, these catchments are considered connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = (a0.T<=a1)&(a0<=a1.T)&(a0!=a0.T)#(a0.T<=a1) means start_date2 <= end_date1\n",
    "                                        #(a0<=a1.T) means start_date1 <= end_date2\n",
    "                                        #(a0!=a0.T) to desconsider the diagonal of matrix - ids connected with themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_start = np.maximum(a0, a0.T) # this function is element-wise comparison between start_date1 and start_date2\n",
    "latest_start = np.where(cond1, latest_start, 0) # we can filter the data based on the first condition, if the condition are False the element gets a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same ideia as the latest_start for the end_date\n",
    "earlist_end = np.minimum(a1, a1.T)\n",
    "earlist_end = np.where(cond1, earlist_end, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = earlist_end - latest_start # difference between both (making sure the operation are between Zeros (elements that doesnt meet the conditions) and TIMEDELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_f = np.where(cond1, a2,0) # filtering the duration based on the condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = np.where(delta>=a2_f/2, delta, 0) # filtering the delta, if the values meet the criteria, then oc takes delta... otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur = oc!=0  # since we do not care about the value itself, but the if the criteria is met or not, we can check if True or False\n",
    "               # remember 0 == False, 1 == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocoor=pd.DataFrame(occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual representation of the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start1 <= end2\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "plt.imshow(a0<=a1.T, cmap=\"PiYG\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"drought events\")\n",
    "plt.ylabel(\"drought events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start2 <= end1\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "plt.imshow(a0.T<=a1, cmap=\"PiYG\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"drought events\")\n",
    "plt.ylabel(\"drought events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimun condition of dates (if minimum overlap occurs) and if delta >= duration/2\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "plt.imshow(occur, cmap=\"PiYG\")\n",
    "plt.xlabel(\"drought events\")\n",
    "plt.ylabel(\"drought events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum condition of dates (if minimum overlap occurs)\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "plt.imshow(cond1, cmap=\"PiYG\" ) \n",
    "plt.xlabel(\"drought events\")\n",
    "plt.ylabel(\"drought events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points that reach minimum condition but failed in delta >= duration/2\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "plt.imshow((cond1-occur.astype(int)), cmap=\"PiYG\" ) \n",
    "plt.xlabel(\"drought events\")\n",
    "plt.ylabel(\"drought events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantity of ids that are connected only in one way\n",
    "np.sum(((a0.T<=a1)&(a0<=a1.T)).astype(int)-occur.astype(int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ids where the condition is True\n",
    "x = np.where(occur)[0] #indices\n",
    "y = np.where(occur)[1] #indices\n",
    "\n",
    "id_1=a3[0][x]\n",
    "id_2=a3[0][y]\n",
    "datex=a1[0][x]\n",
    "datey=a1[0][y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"id_1.npy\", x) #saving x index\n",
    "np.save(\"id_2.npy\", y) #saving y index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform in dataframe\n",
    "id1= pd.DataFrame(id_1)\n",
    "id2= pd.DataFrame(id_2)\n",
    "date_id1 = pd.DataFrame(datex)\n",
    "date_id2 = pd.DataFrame(datey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the dataframe colums\n",
    "id1.rename(columns={0:\"id1\"}, inplace=True)\n",
    "date_id1.rename(columns={0:\"date_id1\"}, inplace=True)\n",
    "date_id2.rename(columns={0:\"date_id2\"}, inplace=True)\n",
    "id2.rename(columns={0:\"id2\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all the information in one dataframe\n",
    "all_ids=pd.DataFrame()\n",
    "all_ids[\"id1\"]=id1[\"id1\"]\n",
    "all_ids[\"id2\"]=id2[\"id2\"]\n",
    "all_ids[\"date_id1\"]=date_id1[\"date_id1\"]\n",
    "all_ids[\"date_id2\"]=date_id2[\"date_id2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the information in a unique file\n",
    "all_ids.to_csv(\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of connections between 2 ids\n",
    "all_ids.loc[(all_ids[\"id1\"]==5)&\n",
    "            (all_ids[\"id2\"]==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group Id 1 and Id2 in two columns\n",
    "size_ids=all_ids.groupby(by=[\"id1\",\"id2\"]).size().sort_index(level=[\"id1\",\"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the informaton of connectedness\n",
    "np.save(\"connectedness.npy\", size_ids)\n",
    "size_ids= pd.DataFrame(size_ids).reset_index()\n",
    "size_ids.rename(columns={0:\"N of connected events\"}, inplace=True)\n",
    "size_ids.to_csv(\"*.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "geopandas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
